{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Github version CIS 545 Homework 2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtien/BFS_Graph_Traversal/blob/master/Spark_SQL_Graph_Traversal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISe_XMH7ivEa",
        "colab_type": "text"
      },
      "source": [
        "#Spark, Hierarchical Data and Graph Data on Yelp Reviews Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJJHiaJ_1P9G",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started with Apache Spark\n",
        "\n",
        "Apache Spark, which has become the de facto successor to Apache Hadoop, is a complex, cluster-based data processing system that was written in Scala.  It leverages a wide variety of distributed tools and components used for big data processing.  It interfaces “smoothly” to Python, but be forewarned that there are some rough edges.  For those interested in why, there are a few reasons:\n",
        "\n",
        "* Scala has slightly different notions of types (especially things like Rows) and handles missing values (nulls) differently from Python.\n",
        "* The Scala-based Spark “engine” can’t just run Python functions as it’s doing data processing.  This means that you want to be careful to use Spark’s library of functions, or the special mechanisms for inserting “user defined functions.”\n",
        "* DataFrames on Spark are “sharded,” so there is no single object corresponding to the DataFrame!\n",
        "\n",
        "While Spark DataFrames try to emulate the same programming style as Pandas DataFrames, there are some differences in how you express things.  Please refer to the Lecture Slides for our take on the differences.  You may also find the following Web pages to be useful resources for understanding Spark vs Pandas DataFrames:\n",
        "\n",
        "https://lab.getbase.com/pandarize-spark-dataframes/\n",
        "https://ogirardot.wordpress.com/2015/07/31/from-pandas-to-apache-sparks-dataframe/ \n",
        "\n",
        "For this assignment, we are going to get familiar with Spark without worrying too much about sharding and distribution.  We are going to run Spark on your Docker container.  This isn’t really using it to its strengths -- and in fact you might find Spark to be unexpectedly slow -- but it will get you comfortable with programming in Spark without worrying about distributed nodes, clusters, and spending real dollars on the cloud.  Your code, if written properly, will “naturally scale” to clusters running on the Cloud.  Later in the term we’ll connect your Jupyter instance to Spark running on the cloud -- to handle “truly big data.”\n",
        "\n",
        "\n",
        "### Initializing a Connection to Spark\n",
        "\n",
        "We'll open a connection to Spark as follows. Note that Spark has multiple interfaces, as you will see if you look at sample code elsewhere. `SparkSession` is the “most modern” one and we’ll be using it for this course.  From `SparkSession`, you can load data into Spark DataFrames as well as `RDD`s.\n",
        "\n",
        "Run the follow cells to setup the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8RH4R771X6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9ab1a76-f88e-47bb-a5ba-6ffede2831a4"
      },
      "source": [
        "!apt install libkrb5-dev\n",
        "!wget https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "! pip install pyspark --user\n",
        "! pip install seaborn --user\n",
        "! pip install plotly --user\n",
        "! pip install imageio --user\n",
        "! pip install folium --user"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libkrb5-dev is already the newest version (1.16-2ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 106 not upgraded.\n",
            "--2020-02-25 00:17:23--  https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
            "Resolving www-us.apache.org (www-us.apache.org)... 40.79.78.1\n",
            "Connecting to www-us.apache.org (www-us.apache.org)|40.79.78.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz [following]\n",
            "--2020-02-25 00:17:23--  https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 2a01:4f8:10a:201a::2\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 232530699 (222M) [application/x-gzip]\n",
            "Saving to: ‘spark-2.4.5-bin-hadoop2.7.tgz.2’\n",
            "\n",
            "spark-2.4.5-bin-had 100%[===================>] 221.76M  19.6MB/s    in 12s     \n",
            "\n",
            "2020-02-25 00:17:36 (17.9 MB/s) - ‘spark-2.4.5-bin-hadoop2.7.tgz.2’ saved [232530699/232530699]\n",
            "\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: sparkmagic in /usr/local/lib/python3.6/dist-packages (0.15.0)\n",
            "Requirement already satisfied: notebook>=4.2 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (4.6.1)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (0.25.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (2.21.0)\n",
            "Requirement already satisfied: requests-kerberos>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (0.12.0)\n",
            "Requirement already satisfied: autovizwidget>=0.6 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (0.15.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (1.3.7)\n",
            "Requirement already satisfied: ipywidgets>5.0.0 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (7.5.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.2 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (5.5.0)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (4.0.1)\n",
            "Requirement already satisfied: hdijupyterutils>=0.6 in /usr/local/lib/python3.6/dist-packages (from sparkmagic) (0.15.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (2.11.1)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (0.8.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (5.6.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (5.0.4)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (5.3.4)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook>=4.2->sparkmagic) (4.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->sparkmagic) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->sparkmagic) (2018.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->sparkmagic) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->sparkmagic) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->sparkmagic) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->sparkmagic) (2.8)\n",
            "Requirement already satisfied: cryptography>=1.3; python_version != \"3.3\" in /usr/local/lib/python3.6/dist-packages (from requests-kerberos>=0.8.0->sparkmagic) (2.8)\n",
            "Requirement already satisfied: pykerberos<2.0.0,>=1.1.8; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from requests-kerberos>=0.8.0->sparkmagic) (1.2.1)\n",
            "Requirement already satisfied: plotly>=3 in /usr/local/lib/python3.6/dist-packages (from autovizwidget>=0.6->sparkmagic) (4.4.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>5.0.0->sparkmagic) (3.5.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (4.4.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (45.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.2->sparkmagic) (0.8.1)\n",
            "Requirement already satisfied: jupyter>=1 in /usr/local/lib/python3.6/dist-packages (from hdijupyterutils>=0.6->sparkmagic) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.2->sparkmagic) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook>=4.2->sparkmagic) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (1.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.2->sparkmagic) (0.3)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook>=4.2->sparkmagic) (2.6.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook>=4.2->sparkmagic) (17.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook>=4.2->sparkmagic) (1.12.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=1.3; python_version != \"3.3\"->requests-kerberos>=0.8.0->sparkmagic) (1.14.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (1.3.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.2->sparkmagic) (0.1.8)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (4.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.2->sparkmagic) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=1.3; python_version != \"3.3\"->requests-kerberos>=0.8.0->sparkmagic) (2.19)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.5)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.5)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.17.5)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.25.3)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.22.0->seaborn) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn) (45.1.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.12.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.17.5)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from folium) (1.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from folium) (2.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from folium) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from folium) (1.17.5)\n",
            "Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from folium) (0.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->folium) (1.1.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->folium) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNctzcXRkexY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "1e494dbb-ae27-4ab6-d801-4912c98888fc"
      },
      "source": [
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.162)] [Waiting for headers] [Wa\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.162)\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.162)\u001b[0m\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\u001b[0m\r                                                                               \rHit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 2s (122 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "106 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "gcc is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "libkrb5-dev is already the newest version (1.16-2ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 106 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP28kxLekWG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName('Graphs-HW2').getOrCreate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gm6aXPq1Ulc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext sparkmagic.magics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiGROEgu1gfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#graph section\n",
        "import networkx as nx\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "# Parallel processing\n",
        "# import swifter\n",
        "\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure\n",
        "\n",
        "import os\n",
        "os.environ['SPARK_HOME'] = '/content/spark-2.4.5-bin-hadoop2.7'\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF0xipwC1hme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw-YbXpG1owp",
        "colab_type": "text"
      },
      "source": [
        "### Download data\n",
        "\n",
        "The following code retrieves the Yelp dataset files from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuRm7t0it3nF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b0edd5f1-447e-48a8-ff58-33b1e88861cb"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1XCANGSCd0pUNcXq18t2QDwCIpJxus8Dy',\n",
        "                                    dest_path='/content/yelp_business_attributes.csv')\n",
        "\n",
        "# gdd.download_file_from_google_drive(file_id='1zY4xbZXPbnahBahraU7ZxoGF3zUZ4SO8',\n",
        "                                    # dest_path='/content/yelp_business_hours.csv')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='11lwBibxX7PYGgOfHU25_dDDDsPX1Pt0Y',\n",
        "                                    dest_path='/content/yelp_business.csv')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1FU5Q-96erhTmk8SjC4XHUm94yWc6h3a0',\n",
        "                                    dest_path='/content/yelp_checkin.csv')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1UaaLrCKjqoQ7G3JT_VUw56pc-dnTwyrS', dest_path='/content/yelp_review2.csv')\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1JNFZeLlimxNSwcOb-oBxxbwJqdg22WgD',\n",
        "                                    dest_path='/content/yelp_user.csv')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1XCANGSCd0pUNcXq18t2QDwCIpJxus8Dy into /content/yelp_business_attributes.csv... Done.\n",
            "Downloading 11lwBibxX7PYGgOfHU25_dDDDsPX1Pt0Y into /content/yelp_business.csv... Done.\n",
            "Downloading 1FU5Q-96erhTmk8SjC4XHUm94yWc6h3a0 into /content/yelp_checkin.csv... Done.\n",
            "Downloading 1UaaLrCKjqoQ7G3JT_VUw56pc-dnTwyrS into /content/yelp_review2.csv... Done.\n",
            "Downloading 1JNFZeLlimxNSwcOb-oBxxbwJqdg22WgD into /content/yelp_user.csv... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8_JCFdZl79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "aa1e1ba6-f46d-4268-d293-9c0652cb25ef"
      },
      "source": [
        "!rm -rf yelp_*\n",
        "!wget https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_business.csv\n",
        "!wget https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_review2.csv\n",
        "!wget https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_business_attributes.csv\n",
        "!wget https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_checkin.csv\n",
        "!wget https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_user.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-25 00:30:09--  https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_business.csv\n",
            "Resolving upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)... 52.216.111.83\n",
            "Connecting to upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)|52.216.111.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30364138 (29M) [text/csv]\n",
            "Saving to: ‘yelp_business.csv’\n",
            "\n",
            "yelp_business.csv   100%[===================>]  28.96M  28.3MB/s    in 1.0s    \n",
            "\n",
            "2020-02-25 00:30:10 (28.3 MB/s) - ‘yelp_business.csv’ saved [30364138/30364138]\n",
            "\n",
            "--2020-02-25 00:30:11--  https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_review2.csv\n",
            "Resolving upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)... 52.216.111.83\n",
            "Connecting to upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)|52.216.111.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3791120545 (3.5G) [text/csv]\n",
            "Saving to: ‘yelp_review2.csv’\n",
            "\n",
            "yelp_review2.csv    100%[===================>]   3.53G  47.2MB/s    in 78s     \n",
            "\n",
            "2020-02-25 00:31:29 (46.4 MB/s) - ‘yelp_review2.csv’ saved [3791120545/3791120545]\n",
            "\n",
            "--2020-02-25 00:31:30--  https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_business_attributes.csv\n",
            "Resolving upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)... 52.217.45.212\n",
            "Connecting to upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)|52.217.45.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41377121 (39M) [text/csv]\n",
            "Saving to: ‘yelp_business_attributes.csv’\n",
            "\n",
            "yelp_business_attri 100%[===================>]  39.46M  28.9MB/s    in 1.4s    \n",
            "\n",
            "2020-02-25 00:31:32 (28.9 MB/s) - ‘yelp_business_attributes.csv’ saved [41377121/41377121]\n",
            "\n",
            "--2020-02-25 00:31:32--  https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_checkin.csv\n",
            "Resolving upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)... 52.216.178.91\n",
            "Connecting to upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)|52.216.178.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 135964892 (130M) [text/csv]\n",
            "Saving to: ‘yelp_checkin.csv’\n",
            "\n",
            "yelp_checkin.csv    100%[===================>] 129.67M  39.5MB/s    in 3.3s    \n",
            "\n",
            "2020-02-25 00:31:36 (39.5 MB/s) - ‘yelp_checkin.csv’ saved [135964892/135964892]\n",
            "\n",
            "--2020-02-25 00:31:37--  https://upenn-bigdataanalytics.s3.amazonaws.com/data/yelp_user.csv\n",
            "Resolving upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)... 54.231.98.88\n",
            "Connecting to upenn-bigdataanalytics.s3.amazonaws.com (upenn-bigdataanalytics.s3.amazonaws.com)|54.231.98.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1363176944 (1.3G) [text/csv]\n",
            "Saving to: ‘yelp_user.csv’\n",
            "\n",
            "yelp_user.csv       100%[===================>]   1.27G  39.9MB/s    in 30s     \n",
            "\n",
            "2020-02-25 00:32:07 (43.0 MB/s) - ‘yelp_user.csv’ saved [1363176944/1363176944]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tjhGYPK1vmm",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Load Our Graph Datasets.\n",
        "\n",
        "For this assignment, we’ll be looking at graph data (reviews, reviewers, businesses) downloaded from Yelp.\n",
        "\n",
        "**A very brief review of graph theory**. Recall that a graph $G$ is composed of a set of vertices $V$ (also called nodes) and edges $E$ (sometimes called links).  Each vertex $v \\in V$ has an identity (often represented in the real world as a string or numeric “node ID”).  Each edge $e \\in E$ is a tuple $(v_i,v_j)$ where $v_i$ represents the source or origin of the edge, and $v_j$ represents the target or destination.  In the simplest case, the edge tuple above is simply the pair $(v_i,v_j)$ but in many cases we may have additional fields such as a label or a distance.  Recall also that graphs may be undirected or directed; in undirected graphs, all edges are symmetric whereas in directed graphs, they are not.  For instance, airline flights are directed, whereas Facebook friend relationships are undirected.\n",
        "\n",
        "Let’s read our social graph data from Yelp, which forms a directed graph.  Here, the set of nodes is also not specified; the assumption is that the only nodes that matter are linked to other nodes, and thus their IDs will appear in the set of edges.  To load the file `input.txt` into a Spark DataFrame, you can use lines like the following.\n",
        "\n",
        "```\n",
        "# Read lines from the text file\n",
        "input_sdf = spark.read.load('input.txt', format=\"text\")\n",
        "```\n",
        "\n",
        "We’ll use the suffix `_sdf` to represent “Spark DataFrame,” much as we used `_df` to denote a Pandas DataFrame in Homework 1.  Load the various files from Yelp.\n",
        "\n",
        "Your datasets should be named `yelp_business_sdf`, `yelp_business_attributes_sdf`, `yelp_check_in_sdf`, `yelp_reviews_sdf`, and `yelp_users_sdf`.\n",
        "\n",
        "Submit the first 75 entries of the yelp_business_sdf to the autograder as a pandas dataframe by using the toPandas() function to convert it from a spark dataframe to a normal dataframe. Also,\n",
        " make sure to sort it by the \"name\" column in ascending order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mtQl7Bi1rHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yelp_business_sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"yelp_business.csv\")\n",
        "#yelp_business_sdf.show()\n",
        "yelp_business = yelp_business_sdf.toPandas()\n",
        "yelp_business = yelp_business.sort_values(by=['name'], ascending=True)\n",
        "#display(yelp_business)\n",
        "\n",
        "yelp_business_attributes_sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"yelp_business_attributes.csv\")\n",
        "yelp_check_in_sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"yelp_checkin.csv\")\n",
        "yelp_reviews_sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"yelp_review2.csv\")\n",
        "yelp_users_sdf = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"yelp_user.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvm3tmQ7c6N-",
        "colab_type": "text"
      },
      "source": [
        "Put spark dataframes into temporary tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADFIa6jx16Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yelp_business_sdf.createOrReplaceTempView('yelp_business')\n",
        "yelp_business_attributes_sdf.createOrReplaceTempView('yelp_business_attributes')\n",
        "yelp_check_in_sdf.createOrReplaceTempView('yelp_check_in')\n",
        "yelp_reviews_sdf.createOrReplaceTempView('yelp_reviews')\n",
        "yelp_users_sdf.createOrReplaceTempView('yelp_users')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DBjwu_4ZbgdG"
      },
      "source": [
        "### 4.2 Simple Analytics on the Data\n",
        "\n",
        "In this section, we shall be executing Spark operations on the data given. Beyond simply executing the queries, you may try using `.explain()` method to see more about the query execution. Also, please read the data description prior to attempting the following questions to understand the data.\n",
        "\n",
        "\n",
        "Compute, stored in `best_average_sdf`, the list of names of businesses based on their average review score (review stars), in decreasing order, sorted lexicographically (in increasing order) by name if they have the same score.  Output the number of reviews also.  Call the columns `name`, `avg_rating`, and `count`.\n",
        "\n",
        "Convert this to pandas and submit the first 75 rows "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_HT_LG1bf1t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "3267dc18-e699-4499-9c99-9ce60c36fcec"
      },
      "source": [
        "#yelp_business_sdf.explain()\n",
        "best_average_sdf_75_first = spark.sql('select name as name, stars avg_rating, review_count as count from yelp_business order by stars desc, name asc limit 75')\n",
        "best_average_sdf_75 = best_average_sdf_75_first.toPandas()\n",
        "display(best_average_sdf_75)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>avg_rating</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"\"\"T\"\"s Hair Affair\"</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2 \"\"Di\"\" 4 Gourmet Karmel Korn\"</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Davis \"\"N\"\" Sons Car Detailing\"</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"Manantial De Salud \"\"The Vitamin Store\"\"\"</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"Scotty\"\"s Kitchen\"</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>24 Hours of Booty</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>24-7 Electrical Services</td>\n",
              "      <td>5.0</td>\n",
              "      <td>124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>24-7 JB Services Garage Door</td>\n",
              "      <td>5.0</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>24/7 Carpet &amp; Floor Care</td>\n",
              "      <td>5.0</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>24/7 Foot Doctor - Podiatry House &amp; Hotel Calls</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               name avg_rating count\n",
              "0                              \"\"\"T\"\"s Hair Affair\"        5.0    10\n",
              "1                  \"2 \"\"Di\"\" 4 Gourmet Karmel Korn\"        5.0     7\n",
              "2                  \"Davis \"\"N\"\" Sons Car Detailing\"        5.0     4\n",
              "3        \"Manantial De Salud \"\"The Vitamin Store\"\"\"        5.0     3\n",
              "4                               \"Scotty\"\"s Kitchen\"        5.0     3\n",
              "..                                              ...        ...   ...\n",
              "70                                24 Hours of Booty        5.0     5\n",
              "71                         24-7 Electrical Services        5.0   124\n",
              "72                     24-7 JB Services Garage Door        5.0    47\n",
              "73                         24/7 Carpet & Floor Care        5.0    53\n",
              "74  24/7 Foot Doctor - Podiatry House & Hotel Calls        5.0    20\n",
              "\n",
              "[75 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2L8pEamDbeUC"
      },
      "source": [
        "#### 4.2.1 Users who are more negative than average\n",
        "\n",
        "Find the users whose average review is below the *average of the per-user* average reviews.  Think about how to factor that into steps!\n",
        "\n",
        "* Compute the (floating-point) variable `overall_avg` as the average of the users' average reviews. (You might compute this in a Spark DataFrame first).\n",
        "* Then output `negative_users_sdf` as the users whose average rating is below that.  This Spark dataframe should have `name` and `avg_rating` and should be sorted first (from lowest to highest) by average rating, then lexicographically (in ascending order) by name.  You should drop cases where the name is null.\n",
        "\n",
        "Submit just the overall_avg number that you get to the autograder as a float\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8X8U1EQebds6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "ccd8866b-4b5b-42c3-a880-5d2e2db8f563"
      },
      "source": [
        "#overall_avg_sdf = yelp_users_sdf.select(F.avg(\"average_stars\")).show()\n",
        "overall_avg = float(3.7108406832064325)\n",
        "\n",
        "negative_users_sdf = spark.sql('select name as name, average_stars as avg_rating from yelp_users where average_stars < 3.7108406832064325 and name is not null order by average_stars desc, name asc limit 75').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "| name|avg_rating|\n",
            "+-----+----------+\n",
            "| -Roc|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|    A|      3.71|\n",
            "|A And|      3.71|\n",
            "|   A.|      3.71|\n",
            "| A.M.|      3.71|\n",
            "| A.M.|      3.71|\n",
            "|   AJ|      3.71|\n",
            "|   AM|      3.71|\n",
            "+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6n8ooy9iIqH",
        "colab_type": "text"
      },
      "source": [
        "### 4.2.2 Cities by number of businesses\n",
        "\n",
        "Find the top 10 cities by number of (Yelp-listed) businesses.\n",
        "\n",
        "This time, use the `take()` function to create a *list* of the top 10 cities (as Rows).  Call this list `top10_cities` and make sure it includes city `name` and `num_restaurants`.\n",
        "\n",
        "We want to answer as a list of lists where each element is the combination of the city name + num_restaurants:\n",
        "\n",
        "[[\"Philadelphia\", 12345], [\"Los Angeles\", 543]]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7lPtnowiH5u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0de668ca-7b31-4bac-f53a-787fcc05801e"
      },
      "source": [
        "city_count_sdf = spark.sql('select city as name, count(*) as num_businesses from yelp_business group by city')\n",
        "restaurant_count_sdf = spark.sql('select city as name, count(categories) as num_restaurants from yelp_business where categories like \"%Restaurants%\" group by city')\n",
        "\n",
        "\n",
        "city_count_sdf.createOrReplaceTempView('city_count')\n",
        "restaurant_count_sdf.createOrReplaceTempView('restaurant_count')\n",
        "\n",
        "join_sdf = spark.sql('select city.name, city.num_businesses, num_restaurants from city_count city left join restaurant_count on city.name=restaurant_count.name order by num_businesses desc limit 10')\n",
        "\n",
        "join_df = join_sdf.toPandas()\n",
        "top10_cities = join_df.take([0,1], axis=1)\n",
        "final = top10_cities.values.tolist()\n",
        "print(final)\n",
        "#yelp_business_sdf.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Las Vegas', 26775], ['Phoenix', 17213], ['Toronto', 17206], ['Charlotte', 8553], ['Scottsdale', 8228], ['Pittsburgh', 6355], ['Mesa', 5760], ['Montréal', 5709], ['Henderson', 4465], ['Tempe', 4263]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np4FWzhmpvRx",
        "colab_type": "text"
      },
      "source": [
        "The following is code we have given you. This builds a relationship graph thaat will be used in the next section. Take a look at it to familiarize yourself with it for the next section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-JlSq4FCbbCh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "09bc6b72-d8b8-4841-e2ce-80a54c06adb4"
      },
      "source": [
        "review_graph_sdf = spark.sql(\"SELECT user_id AS from_node, business_id AS to_node, stars AS score FROM yelp_reviews \"\\\n",
        "                        \"WHERE business_id is not null \"\\\n",
        "                        \"AND user_id is not null\")\n",
        "review_graph_sdf.createOrReplaceTempView('review_graph')\n",
        "\n",
        "review_graph_sdf.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|           from_node|             to_node|score|\n",
            "+--------------------+--------------------+-----+\n",
            "|bv2nCi5Qv5vroFiqK...|AEx2SYEUJmTxVVB18...|    5|\n",
            "|bv2nCi5Qv5vroFiqK...|VR6GpWIda3SfvPC-l...|    5|\n",
            "|bv2nCi5Qv5vroFiqK...|CKC0-MOWMqoeWf6s-...|    5|\n",
            "|bv2nCi5Qv5vroFiqK...|ACFtxLv8pGrrxMm6E...|    4|\n",
            "|bv2nCi5Qv5vroFiqK...|s2I_Ni76bjJNK9yG6...|    4|\n",
            "|_4iMDXbXZ1p1ONG29...|8QWPlVQ6D-OExqXoa...|    5|\n",
            "|u0LXt3Uea_GidxRW1...|9_CGhHMz8698M9-Pk...|    4|\n",
            "|u0LXt3Uea_GidxRW1...|gkCorLgPyQLsptTHa...|    4|\n",
            "|u0LXt3Uea_GidxRW1...|5r6-G9C4YLbC7Ziz5...|    3|\n",
            "|u0LXt3Uea_GidxRW1...|fDF_o2JPU8BR1Gya-...|    5|\n",
            "|u0LXt3Uea_GidxRW1...|z8oIoCT1cXz7gZP5G...|    4|\n",
            "|u0LXt3Uea_GidxRW1...|XWTPNfskXoUL-Lf32...|    3|\n",
            "|u0LXt3Uea_GidxRW1...|13nKUHH-uEUXVZylg...|    1|\n",
            "|u0LXt3Uea_GidxRW1...|RtUvSWO_UZ8V3Wpj0...|    3|\n",
            "|u0LXt3Uea_GidxRW1...|Aov96CM4FZAXeZvKt...|    5|\n",
            "|u0LXt3Uea_GidxRW1...|0W4lkclzZThpx3V65...|    4|\n",
            "|u0LXt3Uea_GidxRW1...|fdnNZMk1NP7ZhL-YM...|    1|\n",
            "|u0LXt3Uea_GidxRW1...|PFPUMF38-lraKzLcT...|    3|\n",
            "|u0LXt3Uea_GidxRW1...|oWTn2IzrprsRkPfUL...|    3|\n",
            "|u0LXt3Uea_GidxRW1...|zgQHtqX0gqMw1nlBZ...|    1|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA84jsFau2Ls",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Part 5. “Traversing” a Graph\n",
        "\n",
        "For our next tasks, we will be “walking” the graph and making connections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrftJPvUiY59",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 5.1 Distributed Breadth-First Search\n",
        "A search algorithm typically starts at a node or set of nodes, and “explores” or “walks” for some number of steps to find a match or a set of matches.\n",
        "\n",
        "If you need an introduction for BFS, I highly suggest looking here: https://www.tutorialspoint.com/data_structures_algorithms/breadth_first_traversal.htm\n",
        "\n",
        "We will be asking for something slightly different tho...\n",
        "\n",
        "Let’s implement a distributed version of a popular algorithm, breadth-first-search (BFS).  This algorithm is given a graph `G`, a set of origin nodes `N`, and a depth `d`.  In each iteration or round up to depth `d`, it explores the set of all new nodes directly connected to the nodes it already has seen, before going on to the nodes another “hop” away.  If we do this correctly, we will explore the graph in a way that (1) avoids getting caught in cycles or loops, and (2) visits each node in the fewest number of “hops” from the origin.  BFS is commonly used in tasks such as friend recommendation in social networks.\n",
        "\n",
        "**How does distributed BFS in Spark work**?  Let’s start with a brief sketch of standard BFS.  During exploration “rounds”, we can divide the graph into three categories:\n",
        "\n",
        "1. *unexplored nodes*.  These are nodes we have not yet visited.  You don’t necessarily need to track these separately from the graph.\n",
        "2. *visited nodes*.  We have already reached these nodes in a previous “round”.\n",
        "3. *frontier nodes*.  These are nodes we have visited in this round.  We have not yet checked whether they have out-edges connecting to unexplored nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er66Vdjjia_W",
        "colab_type": "text"
      },
      "source": [
        "Let’s look at the figure, which shows a digraph.  The green node A represents the origin.\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src = \"https://imgur.com/WU3AUwg.png\" width= \"600\" align =\"center\"/>\n",
        "\n",
        "* In the first round, the origin A is the sole frontier node.  We find all nodes reachable directly from A, namely B-F; then we remove all nodes we have already visited (there are none) or that are in the frontier (the node A itself).  This leaves the blue nodes B-F, which are all reachable in (at most) 1 hop from A.\n",
        "* In the second round, we move A to the visited set and B-F to the frontier.  Now we explore all nodes connected directly to frontier nodes, namely A (from B), F (from E), and the red nodes G-L.  We eliminate the nodes already contained in the frontier and visited sets from the next round’s frontier set, leaving the red nodes only.\n",
        "* In the third round, we will move B-F to the visited set, G-L to the frontier set, and explore the next round of neighbors N-V.  This process continues up to some maximum depth (or until there are no more unexplored nodes).\n",
        "\n",
        "Assume we create data structures (we can make them DataFrames) for the visited and frontier nodes.  Consider (1) how to initialize the different sets at the start of computation (note: unexplored nodes are already in the graph), and (2) how to use the graph edges and the existing data structures to update state for the next iteration “round”.\n",
        "\n",
        "You might possibly have seen how to create a breadth-first-search algorithm in a single-CPU programming language, using a queue to capture the frontier nodes. With Spark we don’t need a queue -- we just need the three sets above.\n",
        "\n",
        "### 5.2 Breadth-First Search Algorithm\n",
        "\n",
        "Create a function `spark_bfs(G, origins, max_depth)` that takes a Spark DataFrame with a graph G (following the schema for `review_graph_sdf` described above, but to be treated as an **undirected graph**), a Python list-of-dictionaries `origins` of the form \n",
        "\n",
        "```\n",
        "[{‘node’: nid1}, \n",
        " {‘node’: nid2}, \n",
        " …]\n",
        "```\n",
        "\n",
        "and a nonnegative integer “exploration depth” `max_depth` (to only run BFS on a tractable portion of the graph).  The `max_depth` will be the maximum number of edge traversals (e.g., the origin is at `max_depth=0`, one hop from the origin is `max_depth=1`, etc.  The function should return a DataFrame containing pairs of the form (node, distance), where the distance is depth at which $n$ was *first* encountered (i.e., the shortest-path distance from the origin nodes).  Note that the origin nodes should also be returned in this Spark DataFrame (with depth 0)!  \n",
        "\n",
        "You can create a new Spark DataFrame with an integer `node` column from the above list of maps `origins`, as follows. This will give you a DataFrame of the nodes to start the BFS at\n",
        "\n",
        "```\n",
        "schema = StructType([\n",
        "            StructField(\"node\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "    my_sdf = spark.createDataFrame(my_list_of_maps, schema)\n",
        "```\n",
        "\n",
        "In this algorithm, be careful in each iteration to keep only the nodes with their shortest distances (you may need to do aggregation or item removal).  You should accumulate all nodes at distances 0, 1, ..., `max_depth`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXkRvJXKiVum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: iterative search over undirected graph\n",
        "\n",
        "#Notes\n",
        "#Build frontier using an iterative process\n",
        "#First frontier is empty\n",
        "#Start with origin\n",
        "#Then put origin into frontier\n",
        "#Notes\n",
        "#Origin = C\n",
        "#frontier = []\n",
        "#visited = []\n",
        "#Have a table of edges and nodes with from and to columns\n",
        "#Do an inner (union) join on the from nodes which are in the frontier with the entire table of edges and nodes \"reviews\"\n",
        "#This will make the smaller table of From To\n",
        "                                      #C   F\n",
        "                                      #C   A\n",
        "#Then, you add what is in the \"to\" column to the next frontier\n",
        "#Add all letters to a visited matrix\n",
        "\n",
        "#However, right now our graph is directed. We want an undirected graph to do a BFS. Thus, we will\n",
        "#flip our from and to columns around to get the inverse path.\n",
        "#Do this by making a copy of the table \"reviews\" (the one that contains all the edges and nodes) and then\n",
        "#renaming the names of the columns. Then add these two tables together\n",
        "#Now you will get C-->F and C-->A if naturally goes A-->C-->F\n",
        "\n",
        "#Now for second hop:\n",
        "#Change frontier to F and A\n",
        "#Now do same thing and inner join with \"reviews\" big table of all graph connections\n",
        "#Use a drop duplicates beacuse you don't want to have C in the frontier again \n",
        "#Look into exclusive outer join\n",
        "#CreateSparkdataframe()\n",
        "\n",
        "\n",
        "#Above but implemented in code\n",
        "#Given a graph\n",
        "#Step 1\n",
        "#origin = []\n",
        "#graph.createTempView('Normal Graph')\n",
        "#graph.create('Flipped Graph')  #now normal graph and flipped graph are the same thing\n",
        "#Now rename columns to what you want\n",
        "#Step2\n",
        "#Slap columns together - call this graph\n",
        "#Step 3\n",
        "#Declare your frontier and visited\n",
        "#Add origin to frontier\n",
        "#4\n",
        "#maxdepth = 3\n",
        "#while depth<maxdepth:\n",
        "  #inner join frontier and graph to get next hops\n",
        "  #union (slaps two things together) 'visited' with the two column of this dataframe\n",
        "  #Remove duplicates\n",
        "  #Take \"to\" column (which is the first step of the traversal) and make it your frontier\n",
        "  #Replace frontier variable with this new data. \n",
        "#Now this will rinse and repeat (go back to beginning of loop)\n",
        "\n",
        "#End notes\n",
        "\n",
        "def spark_bfs(G, origins, max_depth):\n",
        "  #answer_df\n",
        "  schema = StructType([\n",
        "                StructField(\"node\", StringType(), True),\n",
        "                StructField(\"distance\", IntegerType(), False)\n",
        "            ])\n",
        "  initial_answer = [{'node': 'C', 'distance': 0}]\n",
        "  my_sdf = spark.createDataFrame(initial_answer, schema)\n",
        "  #origin_df\n",
        "  origin_schema = StructType([\n",
        "            StructField(\"node\", StringType(), True)\n",
        "        ])\n",
        "  origin_sdf = spark.createDataFrame(origins, origin_schema)\n",
        "  origin_sdf.createOrReplaceTempView('origin')\n",
        "  #initialize first frontier\n",
        "  frontier = spark.sql('Select node as from_node from origin')\n",
        "  frontier.createOrReplaceTempView('frontier')\n",
        "  depth = 1\n",
        "  #create master df that has no direction\n",
        "  G.createOrReplaceTempView('G')\n",
        "  all_connections_sdf = spark.sql('Select from_node, to_node from G union all select +\\\n",
        "   to_node as from_node, from_node as to_node from G')\n",
        "  all_connections_sdf.createOrReplaceTempView('all_connections')\n",
        "  while depth<=max_depth:\n",
        "    next_hop_sdf = spark.sql('Select all.from_node, all.to_node from all_connections +\\\n",
        "    all inner join frontier f on f.from_node=all.from_node')\n",
        "    next_hop_sdf.createOrReplaceTempView('next_hop')\n",
        "    frontier = spark.sql('Select to_node as from_node from next_hop')\n",
        "    dropped_frontier = frontier.groupBy('from_node').count().where('count = 1')\n",
        "    dropped_frontier.createOrReplaceTempView('frontier')\n",
        "    add_answer_sdf = spark.sql('Select from_node as node from frontier')\n",
        "    new_add_answer_sdf = add_answer_sdf.withColumn(\"distance\", F.lit(depth))\n",
        "    new_add_answer_sdf.createOrReplaceTempView('add_answer')\n",
        "    my_sdf.createOrReplaceTempView('my')\n",
        "    my_sdf = spark.sql('Select * from my union all select * from add_answer')\n",
        "\n",
        "    depth = depth+1\n",
        "  \n",
        "  my_sdf.createOrReplaceTempView('my')\n",
        "  my_sdf = spark.sql('Select * from my ORDER BY node limit 75')\n",
        "  my_df = my_sdf.toPandas()\n",
        "\n",
        "\n",
        "  return (my_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4R-lEaQVTZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## toy example\n",
        "simple = [('A', 'B'),\n",
        "         ('A', 'C'),\n",
        "         ('A', 'D'),\n",
        "         ('C', 'F'),\n",
        "         ('B', 'G'),\n",
        "         ('G', 'H'),\n",
        "         ('D', 'E')]\n",
        "data = {'from_node': ['A', 'A', 'A', 'C', 'B', 'G', 'D'],\n",
        "       'to_node': ['B', 'C', 'D', 'F', 'G', 'H', 'E']}\n",
        "uh = pd.DataFrame.from_dict(data)\n",
        "uh_sdf = spark.createDataFrame(uh)\n",
        "smallOrig = [{'node': 'C'}]\n",
        "\n",
        "smallCount = spark_bfs(uh_sdf, smallOrig, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh2TUK2qw3_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "fda096c9-907b-4786-ffcf-ccceff73dc81"
      },
      "source": [
        "smallCount"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>node</th>\n",
              "      <th>distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>D</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>G</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  node  distance\n",
              "0    A         1\n",
              "1    B         2\n",
              "2    C         0\n",
              "3    D         2\n",
              "4    E         3\n",
              "5    F         1\n",
              "6    G         3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYpE6Pp1njJu",
        "colab_type": "text"
      },
      "source": [
        "This is the starting origin node ID for your BFS and the associated call"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVUwgLmhiiAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "orig = [{'node': 'bv2nCi5Qv5vroFiqKGopiw'}]\n",
        "#grab the count\n",
        "count = spark_bfs(review_graph_sdf, orig, 3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}